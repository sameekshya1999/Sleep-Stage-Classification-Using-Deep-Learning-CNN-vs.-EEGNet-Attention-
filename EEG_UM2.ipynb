{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo33jE0z8cyhZPKKDjeFkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameekshya1999/Sleep-Stage-Classification-Using-Deep-Learning-CNN-vs.-EEGNet-Attention-/blob/main/EEG_UM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYiD_zYyCyew"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, constraints\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import tensorflow_probability as tfp\n",
        "import mne\n",
        "import urllib.request\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import gc\n",
        "\n",
        "# Note: We are implementing evidential deep learning manually without external package\n",
        "# For TF Probability, ensure compatible version: pip install tensorflow-probability==0.18.0\n",
        "\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "mne.set_log_level('ERROR')\n",
        "\n",
        "NUM_SUBJECTS = 20\n",
        "NUM_NIGHTS = 2\n",
        "BASE_URL = \"https://physionet.org/files/sleep-edfx/1.0.0/\"\n",
        "TARGET_CHANNELS = ['EEG Fpz-Cz', 'EEG Pz-Oz']\n",
        "EPOCH_DURATION = 30\n",
        "BATCH_SIZE = 16  # Reduced from 32 to mitigate OOM\n",
        "EPOCHS = 20\n",
        "SAMPLING_RATE = 50  # Keep as is; if still OOM, try 25 later\n",
        "TELEMETRY_SUBJECTS = [2, 4, 5, 6, 7, 12, 13]\n",
        "POSSIBLE_HYPNO_LETTERS = 'CHJPUVAEMORW'\n",
        "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
        "\n",
        "def fetch_data(subject_id, night, record_type='PSG'):\n",
        "    try:\n",
        "        dataset_id = subject_id\n",
        "        folder = \"sleep-cassette\" if night == 1 else \"sleep-telemetry\"\n",
        "        if night == 1:\n",
        "            prefix = f\"SC4{dataset_id:02d}\"\n",
        "        else:\n",
        "            if subject_id not in TELEMETRY_SUBJECTS:\n",
        "                return None\n",
        "            telemetry_map = {2: 702, 4: 704, 5: 705, 6: 706, 7: 707, 12: 712, 13: 713}\n",
        "            prefix = f\"ST{telemetry_map.get(subject_id, 700 + dataset_id)}\"\n",
        "        os.makedirs(\"sleep_edf\", exist_ok=True)\n",
        "        if record_type == 'PSG':\n",
        "            base_suffix = \"E\" if night == 1 else \"J\"\n",
        "            file_name = f\"{prefix}{night if night == 1 else 2}{base_suffix}0-PSG.edf\"\n",
        "            url = f\"{BASE_URL}{folder}/{file_name}\"\n",
        "            local_file = os.path.join(\"sleep_edf\", file_name)\n",
        "            if os.path.exists(local_file):\n",
        "                return local_file\n",
        "            urllib.request.urlretrieve(url, local_file)\n",
        "            print(f\"Downloaded {file_name}\")\n",
        "            return local_file\n",
        "        else:\n",
        "            base_suffix = \"E\" if night == 1 else \"J\"\n",
        "            for letter in POSSIBLE_HYPNO_LETTERS:\n",
        "                hypno_suffix = base_suffix + letter\n",
        "                file_name = f\"{prefix}{night if night == 1 else 2}{hypno_suffix}-Hypnogram.edf\"\n",
        "                url = f\"{BASE_URL}{folder}/{file_name}\"\n",
        "                local_file = os.path.join(\"sleep_edf\", file_name)\n",
        "                if os.path.exists(local_file):\n",
        "                    return local_file\n",
        "                try:\n",
        "                    urllib.request.urlretrieve(url, local_file)\n",
        "                    print(f\"Downloaded {file_name}\")\n",
        "                    return local_file\n",
        "                except urllib.error.HTTPError as e:\n",
        "                    if e.code != 404:\n",
        "                        raise\n",
        "            return None\n",
        "    except urllib.error.HTTPError as e:\n",
        "        print(f\"HTTP Error {e.code} fetching {file_name if 'file_name' in locals() else 'file'}: {e.reason}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {file_name if 'file_name' in locals() else 'file'}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_available_subjects():\n",
        "    available = []\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "        for subject_id in range(NUM_SUBJECTS):\n",
        "            for night in range(1, NUM_NIGHTS + 1):\n",
        "                futures.append((\n",
        "                    subject_id,\n",
        "                    night,\n",
        "                    executor.submit(\n",
        "                        lambda s, n: (\n",
        "                            fetch_data(s, n, 'PSG') is not None and\n",
        "                            fetch_data(s, n, 'Hypnogram') is not None\n",
        "                        ),\n",
        "                        subject_id, night\n",
        "                    )\n",
        "                ))\n",
        "        for subject_id, night, future in tqdm(futures, desc=\"Checking availability\"):\n",
        "            if future.result():\n",
        "                available.append((subject_id, night))\n",
        "    print(f\"Available subject-night pairs: {available}\")\n",
        "    return available\n",
        "\n",
        "def augment_data(X):\n",
        "    noise = np.random.normal(0, 0.01, X.shape)\n",
        "    shift = np.random.randint(-50, 50)\n",
        "    X_aug = np.roll(X + noise, shift, axis=1)\n",
        "    return X_aug\n",
        "\n",
        "def process_subject_night(subject_id, night):\n",
        "    try:\n",
        "        psg_file = fetch_data(subject_id, night, 'PSG')\n",
        "        hypno_file = fetch_data(subject_id, night, 'Hypnogram')\n",
        "        if psg_file is None or hypno_file is None:\n",
        "            print(f\"Skipping subject {subject_id}, night {night}: Missing files\")\n",
        "            return None, None\n",
        "        raw = mne.io.read_raw_edf(psg_file, preload=False, verbose=False)\n",
        "        available_channels = [ch for ch in TARGET_CHANNELS if ch in raw.ch_names]\n",
        "        if not available_channels:\n",
        "            print(f\"No target channels for subject {subject_id}, night {night}\")\n",
        "            return None, None\n",
        "        raw.pick_channels(available_channels)\n",
        "        raw.load_data()\n",
        "        raw.filter(0.5, 40.0, l_trans_bandwidth=0.5, h_trans_bandwidth=10.0, verbose=False)\n",
        "        raw.resample(SAMPLING_RATE, npad=\"auto\")\n",
        "        events = mne.make_fixed_length_events(raw, id=1, duration=EPOCH_DURATION)\n",
        "        epochs_mne = mne.Epochs(raw, events, tmin=0, tmax=EPOCH_DURATION-1/raw.info['sfreq'],\n",
        "                                picks=available_channels, baseline=None, preload=True)\n",
        "        data = epochs_mne.get_data(units='uV')\n",
        "        annotations = mne.read_annotations(hypno_file)\n",
        "        labels = np.zeros(len(epochs_mne), dtype=int)\n",
        "        stage_map = {'Sleep stage W': 0, 'Sleep stage 1': 1, 'Sleep stage 2': 2, 'Sleep stage 3': 3, 'Sleep stage 4': 3, 'Sleep stage R': 4}\n",
        "        for annot in annotations:\n",
        "            onset = int(annot['onset'] / EPOCH_DURATION)\n",
        "            duration = int(annot['duration'] / EPOCH_DURATION)\n",
        "            stage = annot['description']\n",
        "            if stage in stage_map:\n",
        "                for i in range(max(0, onset), min(len(epochs_mne), onset + duration)):\n",
        "                    labels[i] = stage_map[stage]\n",
        "        data = (data - np.mean(data, axis=(1, 2), keepdims=True)) / np.std(data, axis=(1, 2), keepdims=True)\n",
        "        X = data.transpose(0, 2, 1)\n",
        "        X_aug = augment_data(X)\n",
        "        X = np.concatenate([X, X_aug])\n",
        "        labels = np.concatenate([labels, labels])\n",
        "        del raw, epochs_mne, data\n",
        "        gc.collect()\n",
        "        print(f\"Processed subject {subject_id}, night {night}: {X.shape[0]} epochs\")\n",
        "        return X, labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing subject {subject_id}, night {night}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, X, y, batch_size, augment=True, class_weights=None):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.int32)\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.class_weights = class_weights\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.batch_size\n",
        "        end = min(start + self.batch_size, len(self.X))\n",
        "        X_batch = self.X[start:end]\n",
        "        y_batch = self.y[start:end]\n",
        "        if self.augment:\n",
        "            X_batch = augment_data(X_batch).astype(np.float32)\n",
        "        sample_weights = np.ones_like(y_batch, dtype=np.float32)\n",
        "        if self.class_weights:\n",
        "            sample_weights = np.array([self.class_weights[label] for label in y_batch], dtype=np.float32)\n",
        "        return X_batch, y_batch, sample_weights\n",
        "\n",
        "# Custom Evidential Loss\n",
        "def evidential_classification_loss(y_true, evidence):\n",
        "    alpha = evidence + 1.0\n",
        "    S = tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
        "    prob = alpha / S\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=evidence.shape[1])\n",
        "    a = y_true * (1 - y_true) * (y_true - prob) ** 2\n",
        "    b = alpha * (S - alpha) / (S * (S + 1))\n",
        "    mse_var = tf.reduce_mean(tf.reduce_sum(a + b, axis=1))\n",
        "\n",
        "    # Simple MSE + Var without KL for now\n",
        "    return mse_var\n",
        "\n",
        "# Base LSTM Model\n",
        "def build_lstm_model(input_shape, nb_classes=5):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(128, return_sequences=True)(inputs)\n",
        "    x = layers.DepthwiseConv1D(input_shape[1], depth_multiplier=2, padding='same', use_bias=False, depthwise_constraint=constraints.max_norm(1.0))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=4)(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.SeparableConv1D(64, 16, padding='same', use_bias=False, depthwise_constraint=constraints.max_norm(1.0))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=4)(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(nb_classes, activation='softmax', dtype='float32')(x)\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# EEGNet Model\n",
        "def build_eegnet_model(input_shape, nb_classes=5, chans=2, samples=1500, dropout_rate=0.25, kern_length=64, f1=8, d=2, f2=16):\n",
        "    input1 = layers.Input(shape=(samples, chans))  # (time, channels)\n",
        "    x = tf.expand_dims(input1, axis=-1)  # Add channel dimension for Conv2D: (time, chans, 1)\n",
        "    x = layers.Permute((2, 1, 3))(x)  # To (chans, time, 1)\n",
        "\n",
        "    x = layers.Conv2D(f1, (1, kern_length), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.DepthwiseConv2D((chans, 1), use_bias=False, depth_multiplier=d, depthwise_constraint=constraints.max_norm(1.0))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.AveragePooling2D((1, 4))(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.SeparableConv2D(f2, (1, 16), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.AveragePooling2D((1, 8))(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(nb_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = models.Model(inputs=input1, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Batched prediction function to avoid OOM\n",
        "def batched_predict(model, X, batch_size=BATCH_SIZE, training=False):\n",
        "    preds = []\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch = X[i:i+batch_size]\n",
        "        pred = model(batch, training=training)\n",
        "        preds.append(pred.numpy())  # Convert to numpy for concatenation\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "# Monte Carlo Dropout Version\n",
        "def build_mc_dropout_model(base_builder, input_shape, nb_classes=5, dropout_rate=0.25):\n",
        "    model = base_builder(input_shape, nb_classes)\n",
        "    return model\n",
        "\n",
        "def predict_with_mc_dropout(model, X, n_samples=20, batch_size=BATCH_SIZE):  # Reduced n_samples\n",
        "    preds = []\n",
        "    for _ in range(n_samples):\n",
        "        pred = batched_predict(model, X, batch_size, training=True)\n",
        "        preds.append(pred)\n",
        "    preds = np.stack(preds, axis=0)\n",
        "    mean_pred = np.mean(preds, axis=0)\n",
        "    uncertainty = np.std(preds, axis=0)\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Deep Ensembles\n",
        "def train_deep_ensembles(base_builder, input_shape, nb_classes, train_gen, val_gen, n_ensembles=5):\n",
        "    ensembles = []\n",
        "    for _ in range(n_ensembles):\n",
        "        model = base_builder(input_shape, nb_classes)\n",
        "        model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, verbose=1)\n",
        "        ensembles.append(model)\n",
        "    return ensembles\n",
        "\n",
        "def predict_with_deep_ensembles(ensembles, X, batch_size=BATCH_SIZE):\n",
        "    preds = []\n",
        "    for model in ensembles:\n",
        "        pred = batched_predict(model, X, batch_size)\n",
        "        preds.append(pred)\n",
        "    preds = np.stack(preds, axis=0)\n",
        "    mean_pred = np.mean(preds, axis=0)\n",
        "    uncertainty = np.std(preds, axis=0)\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Variational Inference\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    return tfp.layers.default_multivariate_normal_fn\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    return tf.keras.Sequential([\n",
        "        tfp.layers.VariableLayer(tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype),\n",
        "        tfp.layers.MultivariateNormalTriL(n),\n",
        "    ])\n",
        "\n",
        "def build_variational_model(base_builder, input_shape, nb_classes, train_size):\n",
        "    base_model = base_builder(input_shape, nb_classes)\n",
        "    x = base_model.layers[-3].output  # Before last dense and dropout\n",
        "    x = tfp.layers.DenseVariational(\n",
        "        nb_classes,\n",
        "        make_prior_fn=prior,\n",
        "        make_posterior_fn=posterior,\n",
        "        kl_weight=1/train_size,\n",
        "        activation='softmax'\n",
        "    )(x)\n",
        "    model = models.Model(inputs=base_model.input, outputs=x)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def predict_with_variational(model, X, n_samples=20, batch_size=BATCH_SIZE):  # Reduced n_samples\n",
        "    preds = []\n",
        "    for _ in range(n_samples):\n",
        "        pred = batched_predict(model, X, batch_size)\n",
        "        preds.append(pred)\n",
        "    preds = np.stack(preds, axis=0)\n",
        "    mean_pred = np.mean(preds, axis=0)\n",
        "    uncertainty = np.std(preds, axis=0)\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Evidential Deep Learning (manual implementation)\n",
        "def build_evidential_model(base_builder, input_shape, nb_classes):\n",
        "    base_model = base_builder(input_shape, nb_classes)\n",
        "    x = base_model.layers[-2].output  # Before the last Dense\n",
        "    evidence = layers.Dense(nb_classes)(x)\n",
        "    outputs = layers.Activation('relu', dtype='float32')(evidence)  # Ensure non-negative evidence\n",
        "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss=evidential_classification_loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def predict_with_evidential(model, X, batch_size=BATCH_SIZE):\n",
        "    evidence = batched_predict(model, X, batch_size)\n",
        "    alpha = evidence + 1\n",
        "    S = np.sum(alpha, axis=1, keepdims=True)\n",
        "    mean_pred = alpha / S\n",
        "    uncertainty = nb_classes / S\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Uncertainty Metrics\n",
        "def negative_log_likelihood(y_true, y_pred):\n",
        "    y_true_onehot = tf.keras.utils.to_categorical(y_true, num_classes=y_pred.shape[1])\n",
        "    return -np.mean(np.sum(y_true_onehot * np.log(y_pred + 1e-10), axis=1))\n",
        "\n",
        "def brier_score(y_true, y_pred):\n",
        "    y_true_onehot = tf.keras.utils.to_categorical(y_true, num_classes=y_pred.shape[1])\n",
        "    return np.mean(np.sum((y_pred - y_true_onehot)**2, axis=1))\n",
        "\n",
        "def expected_calibration_error(y_true, y_pred, num_bins=15):\n",
        "    pred_classes = np.argmax(y_pred, axis=1)\n",
        "    prob_max = np.max(y_pred, axis=1)\n",
        "    correct = (pred_classes == y_true).astype(np.float32)\n",
        "\n",
        "    bins = np.linspace(0, 1, num_bins + 1)\n",
        "    indices = np.digitize(prob_max, bins, right=True)\n",
        "\n",
        "    ece = 0\n",
        "    for b in range(1, num_bins + 1):\n",
        "        mask = (indices == b)\n",
        "        if np.any(mask):\n",
        "            acc = np.mean(correct[mask])\n",
        "            conf = np.mean(prob_max[mask])\n",
        "            ece += np.abs(acc - conf) * np.sum(mask) / len(y_true)\n",
        "    return ece\n",
        "\n",
        "def plot_training_curves(history, model_name):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_curves_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_model(model_name, predict_func, X_test, y_test):\n",
        "    mean_pred, uncertainty = predict_func(X_test)\n",
        "    test_acc = np.mean(np.argmax(mean_pred, axis=1) == y_test)\n",
        "    nll = negative_log_likelihood(y_test, mean_pred)\n",
        "    brier = brier_score(y_test, mean_pred)\n",
        "    ece = expected_calibration_error(y_test, mean_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} - Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"{model_name} - NLL: {nll:.4f}\")\n",
        "    print(f\"{model_name} - Brier Score: {brier:.4f}\")\n",
        "    print(f\"{model_name} - ECE: {ece:.4f}\")\n",
        "\n",
        "    y_pred_classes = np.argmax(mean_pred, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_classes, average=None)\n",
        "    stage_names = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
        "    print(f\"\\n{model_name} - Per-class Metrics:\")\n",
        "    for i, stage in enumerate(stage_names):\n",
        "        print(f\"{stage}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}\")\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage_names, yticklabels=stage_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return {'acc': test_acc, 'nll': nll, 'brier': brier, 'ece': ece}\n",
        "\n",
        "def data_generator(available, batch_size=2000):\n",
        "    for subject_id, night in available:\n",
        "        X, y = process_subject_night(subject_id, night)\n",
        "        if X is None or y is None:\n",
        "            continue\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "        del X, y\n",
        "        gc.collect()\n",
        "\n",
        "def run_pipeline():\n",
        "    available = get_available_subjects()\n",
        "    if not available:\n",
        "        return\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "    for X_batch, y_batch in tqdm(data_generator(available), desc=\"Processing data\"):\n",
        "        if X_batch is None or y_batch is None:\n",
        "            continue\n",
        "        class_counts = np.bincount(y_batch)\n",
        "        stratify = y_batch if min(class_counts[class_counts > 0]) >= 2 else None\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X_batch, y_batch, test_size=0.2, stratify=stratify, random_state=42)\n",
        "        X_train.append(X_tr); y_train.append(y_tr)\n",
        "        X_test.append(X_te); y_test.append(y_te)\n",
        "        del X_batch, y_batch\n",
        "        gc.collect()\n",
        "    if not X_train:\n",
        "        return\n",
        "    X_train = np.concatenate(X_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    X_test = np.concatenate(X_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "    train_generator = DataGenerator(X_train, y_train, BATCH_SIZE, augment=True, class_weights=class_weight_dict)\n",
        "    val_generator = DataGenerator(X_test, y_test, BATCH_SIZE, augment=False, class_weights=class_weight_dict)\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    nb_classes = 5\n",
        "    train_size = len(X_train)\n",
        "\n",
        "    # Models to compare\n",
        "    base_builders = {'LSTM': build_lstm_model, 'EEGNet': build_eegnet_model}\n",
        "    uncertainty_methods = [\n",
        "        ('MC_Dropout', build_mc_dropout_model, lambda model, X: predict_with_mc_dropout(model, X)),\n",
        "        ('Deep_Ensembles', train_deep_ensembles, lambda ensembles, X: predict_with_deep_ensembles(ensembles, X)),\n",
        "        ('Variational_Inference', lambda bs, ish, nbc: build_variational_model(bs, ish, nbc, train_size), lambda model, X: predict_with_variational(model, X)),\n",
        "        ('Evidential_DL', build_evidential_model, lambda model, X: predict_with_evidential(model, X))\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for base_name, base_builder in base_builders.items():\n",
        "        for unc_name, build_func, predict_func in uncertainty_methods:\n",
        "            model_name = f\"{base_name}_{unc_name}\"\n",
        "            print(f\"\\nTraining {model_name}...\")\n",
        "            if unc_name == 'Deep_Ensembles':\n",
        "                models = build_func(base_builder, input_shape, nb_classes, train_generator, val_generator)\n",
        "                metrics = evaluate_model(model_name, lambda X: predict_func(models, X), X_test, y_test)\n",
        "            else:\n",
        "                model = build_func(base_builder, input_shape, nb_classes)\n",
        "                history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "                plot_training_curves(history, model_name)\n",
        "                metrics = evaluate_model(model_name, lambda X: predict_func(model, X), X_test, y_test)\n",
        "            results[model_name] = metrics\n",
        "\n",
        "    # Compare\n",
        "    print(\"\\nModel Comparison for Uncertainty Estimation:\")\n",
        "    print(\"Sorted by ECE (lower is better):\")\n",
        "    for name, met in sorted(results.items(), key=lambda x: x[1]['ece']):\n",
        "        print(f\"{name}: Acc={met['acc']:.4f}, NLL={met['nll']:.4f}, Brier={met['brier']:.4f}, ECE={met['ece']:.4f}\")\n",
        "\n",
        "    best_model = min(results, key=lambda k: results[k]['ece'])\n",
        "    print(f\"\\nBest fit model for uncertainty estimation: {best_model} with ECE {results[best_model]['ece']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "Sleep Stage Uncertainty Analysis with LSTM\n",
        "LSTM_MC_Dropout - Test Accuracy: 0.8935\n",
        "LSTM_MC_Dropout - NLL: 0.2950\n",
        "LSTM_MC_Dropout - Brier Score: 0.1529\n",
        "LSTM_MC_Dropout - ECE: 0.0082\n",
        "\n",
        "LSTM_MC_Dropout - Per-class Metrics:\n",
        "Wake: Precision=0.9991, Recall=0.9362, F1=0.9666\n",
        "N1: Precision=0.2907, Recall=0.6456, F1=0.4009\n",
        "N2: Precision=0.9184, Recall=0.8105, F1=0.8611\n",
        "N3: Precision=0.8210, Recall=0.8865, F1=0.8525\n",
        "REM: Precision=0.7084, Recall=0.8833, F1=0.7862\n",
        "\n",
        "Training LSTM_Deep_Ensembles...\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 306s 51ms/step - accuracy: 0.7489 - loss: 0.8415 - val_accuracy: 0.8415 - val_loss: 0.6857\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 301s 51ms/step - accuracy: 0.8660 - loss: 0.5438 - val_accuracy: 0.8834 - val_loss: 0.5891\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 301s 51ms/step - accuracy: 0.8823 - loss: 0.4774 - val_accuracy: 0.8081 - val_loss: 0.5846\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 304s 51ms/step - accuracy: 0.7431 - loss: 0.8535 - val_accuracy: 0.8668 - val_loss: 0.5671\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 301s 51ms/step - accuracy: 0.8676 - loss: 0.5359 - val_accuracy: 0.8788 - val_loss: 0.5593\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 302s 51ms/step - accuracy: 0.8812 - loss: 0.4700 - val_accuracy: 0.7630 - val_loss: 0.7458\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 305s 51ms/step - accuracy: 0.7502 - loss: 0.8433 - val_accuracy: 0.8424 - val_loss: 0.5338\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 302s 51ms/step - accuracy: 0.8647 - loss: 0.5448 - val_accuracy: 0.8646 - val_loss: 0.6970\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 302s 51ms/step - accuracy: 0.8819 - loss: 0.4792 - val_accuracy: 0.8865 - val_loss: 0.5155\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 303s 50ms/step - accuracy: 0.7624 - loss: 0.8292 - val_accuracy: 0.7645 - val_loss: 0.7406\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 300s 50ms/step - accuracy: 0.8664 - loss: 0.5302 - val_accuracy: 0.8510 - val_loss: 0.7613\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 300s 50ms/step - accuracy: 0.8853 - loss: 0.4592 - val_accuracy: 0.8575 - val_loss: 0.5485\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 303s 50ms/step - accuracy: 0.7569 - loss: 0.8373 - val_accuracy: 0.8379 - val_loss: 0.5583\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 299s 50ms/step - accuracy: 0.8707 - loss: 0.5175 - val_accuracy: 0.8343 - val_loss: 0.6014\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 300s 50ms/step - accuracy: 0.8846 - loss: 0.4669 - val_accuracy: 0.7968 - val_loss: 0.5733\n",
        "\n",
        "LSTM_Deep_Ensembles - Test Accuracy: 0.8445\n",
        "LSTM_Deep_Ensembles - NLL: 0.4029\n",
        "LSTM_Deep_Ensembles - Brier Score: 0.2132\n",
        "LSTM_Deep_Ensembles - ECE: 0.0157\n",
        "\n",
        "LSTM_Deep_Ensembles - Per-class Metrics:\n",
        "Wake: Precision=0.9998, Recall=0.8923, F1=0.9430\n",
        "N1: Precision=0.2050, Recall=0.8495, F1=0.3302\n",
        "N2: Precision=0.9298, Recall=0.7645, F1=0.8391\n",
        "N3: Precision=0.6904, Recall=0.9483, F1=0.7991\n",
        "REM: Precision=0.7596, Recall=0.6133, F1=0.6787"
      ],
      "metadata": {
        "id": "21abfhJdC1V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, constraints\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import tensorflow_probability as tfp\n",
        "import mne\n",
        "import urllib.request\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import gc\n",
        "\n",
        "# Note: Ensure compatible version: pip install tensorflow-probability==0.18.0\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "mne.set_log_level('ERROR')\n",
        "\n",
        "NUM_SUBJECTS = 20\n",
        "NUM_NIGHTS = 2\n",
        "BASE_URL = \"https://physionet.org/files/sleep-edfx/1.0.0/\"\n",
        "TARGET_CHANNELS = ['EEG Fpz-Cz', 'EEG Pz-Oz']\n",
        "EPOCH_DURATION = 30\n",
        "BATCH_SIZE = 16  # Reduced to mitigate OOM\n",
        "EPOCHS = 3\n",
        "SAMPLING_RATE = 50\n",
        "TELEMETRY_SUBJECTS = [2, 4, 5, 6, 7, 12, 13]\n",
        "POSSIBLE_HYPNO_LETTERS = 'CHJPUVAEMORW'\n",
        "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
        "\n",
        "def fetch_data(subject_id, night, record_type='PSG'):\n",
        "    try:\n",
        "        dataset_id = subject_id\n",
        "        folder = \"sleep-cassette\" if night == 1 else \"sleep-telemetry\"\n",
        "        if night == 1:\n",
        "            prefix = f\"SC4{dataset_id:02d}\"\n",
        "        else:\n",
        "            if subject_id not in TELEMETRY_SUBJECTS:\n",
        "                return None\n",
        "            telemetry_map = {2: 702, 4: 704, 5: 705, 6: 706, 7: 707, 12: 712, 13: 713}\n",
        "            prefix = f\"ST{telemetry_map.get(subject_id, 700 + dataset_id)}\"\n",
        "        os.makedirs(\"sleep_edf\", exist_ok=True)\n",
        "        if record_type == 'PSG':\n",
        "            base_suffix = \"E\" if night == 1 else \"J\"\n",
        "            file_name = f\"{prefix}{night if night == 1 else 2}{base_suffix}0-PSG.edf\"\n",
        "            url = f\"{BASE_URL}{folder}/{file_name}\"\n",
        "            local_file = os.path.join(\"sleep_edf\", file_name)\n",
        "            if os.path.exists(local_file):\n",
        "                return local_file\n",
        "            urllib.request.urlretrieve(url, local_file)\n",
        "            print(f\"Downloaded {file_name}\")\n",
        "            return local_file\n",
        "        else:\n",
        "            base_suffix = \"E\" if night == 1 else \"J\"\n",
        "            for letter in POSSIBLE_HYPNO_LETTERS:\n",
        "                hypno_suffix = base_suffix + letter\n",
        "                file_name = f\"{prefix}{night if night == 1 else 2}{hypno_suffix}-Hypnogram.edf\"\n",
        "                url = f\"{BASE_URL}{folder}/{file_name}\"\n",
        "                local_file = os.path.join(\"sleep_edf\", file_name)\n",
        "                if os.path.exists(local_file):\n",
        "                    return local_file\n",
        "                try:\n",
        "                    urllib.request.urlretrieve(url, local_file)\n",
        "                    print(f\"Downloaded {file_name}\")\n",
        "                    return local_file\n",
        "                except urllib.error.HTTPError as e:\n",
        "                    if e.code != 404:\n",
        "                        raise\n",
        "            return None\n",
        "    except urllib.error.HTTPError as e:\n",
        "        print(f\"HTTP Error {e.code} fetching {file_name if 'file_name' in locals() else 'file'}: {e.reason}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {file_name if 'file_name' in locals() else 'file'}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_available_subjects():\n",
        "    available = []\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "        for subject_id in range(NUM_SUBJECTS):\n",
        "            for night in range(1, NUM_NIGHTS + 1):\n",
        "                futures.append((\n",
        "                    subject_id,\n",
        "                    night,\n",
        "                    executor.submit(\n",
        "                        lambda s, n: (\n",
        "                            fetch_data(s, n, 'PSG') is not None and\n",
        "                            fetch_data(s, n, 'Hypnogram') is not None\n",
        "                        ),\n",
        "                        subject_id, night\n",
        "                    )\n",
        "                ))\n",
        "        for subject_id, night, future in tqdm(futures, desc=\"Checking availability\"):\n",
        "            if future.result():\n",
        "                available.append((subject_id, night))\n",
        "    print(f\"Available subject-night pairs: {available}\")\n",
        "    return available\n",
        "\n",
        "def augment_data(X):\n",
        "    noise = np.random.normal(0, 0.01, X.shape)\n",
        "    shift = np.random.randint(-50, 50)\n",
        "    X_aug = np.roll(X + noise, shift, axis=1)\n",
        "    return X_aug\n",
        "\n",
        "def process_subject_night(subject_id, night):\n",
        "    try:\n",
        "        psg_file = fetch_data(subject_id, night, 'PSG')\n",
        "        hypno_file = fetch_data(subject_id, night, 'Hypnogram')\n",
        "        if psg_file is None or hypno_file is None:\n",
        "            print(f\"Skipping subject {subject_id}, night {night}: Missing files\")\n",
        "            return None, None\n",
        "        raw = mne.io.read_raw_edf(psg_file, preload=False, verbose=False)\n",
        "        available_channels = [ch for ch in TARGET_CHANNELS if ch in raw.ch_names]\n",
        "        if not available_channels:\n",
        "            print(f\"No target channels for subject {subject_id}, night {night}\")\n",
        "            return None, None\n",
        "        raw.pick_channels(available_channels)\n",
        "        raw.load_data()\n",
        "        raw.filter(0.5, 40.0, l_trans_bandwidth=0.5, h_trans_bandwidth=10.0, verbose=False)\n",
        "        raw.resample(SAMPLING_RATE, npad=\"auto\")\n",
        "        events = mne.make_fixed_length_events(raw, id=1, duration=EPOCH_DURATION)\n",
        "        epochs_mne = mne.Epochs(raw, events, tmin=0, tmax=EPOCH_DURATION-1/raw.info['sfreq'],\n",
        "                                picks=available_channels, baseline=None, preload=True)\n",
        "        data = epochs_mne.get_data(units='uV')\n",
        "        annotations = mne.read_annotations(hypno_file)\n",
        "        labels = np.zeros(len(epochs_mne), dtype=int)\n",
        "        stage_map = {'Sleep stage W': 0, 'Sleep stage 1': 1, 'Sleep stage 2': 2, 'Sleep stage 3': 3, 'Sleep stage 4': 3, 'Sleep stage R': 4}\n",
        "        for annot in annotations:\n",
        "            onset = int(annot['onset'] / EPOCH_DURATION)\n",
        "            duration = int(annot['duration'] / EPOCH_DURATION)\n",
        "            stage = annot['description']\n",
        "            if stage in stage_map:\n",
        "                for i in range(max(0, onset), min(len(epochs_mne), onset + duration)):\n",
        "                    labels[i] = stage_map[stage]\n",
        "        data = (data - np.mean(data, axis=(1, 2), keepdims=True)) / np.std(data, axis=(1, 2), keepdims=True)\n",
        "        X = data.transpose(0, 2, 1)\n",
        "        X_aug = augment_data(X)\n",
        "        X = np.concatenate([X, X_aug])\n",
        "        labels = np.concatenate([labels, labels])\n",
        "        del raw, epochs_mne, data\n",
        "        gc.collect()\n",
        "        print(f\"Processed subject {subject_id}, night {night}: {X.shape[0]} epochs\")\n",
        "        return X, labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing subject {subject_id}, night {night}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, X, y, batch_size, augment=True, class_weights=None):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.int32)\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.class_weights = class_weights\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.batch_size\n",
        "        end = min(start + self.batch_size, len(self.X))\n",
        "        X_batch = self.X[start:end]\n",
        "        y_batch = self.y[start:end]\n",
        "        if self.augment:\n",
        "            X_batch = augment_data(X_batch).astype(np.float32)\n",
        "        sample_weights = np.ones_like(y_batch, dtype=np.float32)\n",
        "        if self.class_weights:\n",
        "            sample_weights = np.array([self.class_weights[label] for label in y_batch], dtype=np.float32)\n",
        "        return X_batch, y_batch, sample_weights\n",
        "\n",
        "# Custom Evidential Loss\n",
        "def evidential_classification_loss(y_true, evidence):\n",
        "    alpha = evidence + 1.0\n",
        "    S = tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
        "    prob = alpha / S\n",
        "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=evidence.shape[1])\n",
        "    a = y_true * (1 - y_true) * (y_true - prob) ** 2\n",
        "    b = alpha * (S - alpha) / (S * (S + 1))\n",
        "    mse_var = tf.reduce_mean(tf.reduce_sum(a + b, axis=1))\n",
        "    return mse_var\n",
        "\n",
        "# Base LSTM Model\n",
        "def build_lstm_model(input_shape, nb_classes=5):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.LSTM(128, return_sequences=True)(inputs)\n",
        "    x = layers.DepthwiseConv1D(input_shape[1], depth_multiplier=2, padding='same', use_bias=False, depthwise_constraint=constraints.max_norm(1.0))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=4)(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.SeparableConv1D(64, 16, padding='same', use_bias=False, depthwise_constraint=constraints.max_norm(1.0))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=4)(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(128, activation='relu', name='dense_before_dropout')(x)\n",
        "    x = layers.Dropout(0.3, name='final_dropout')(x)\n",
        "    outputs = layers.Dense(nb_classes, activation='softmax', dtype='float32', name='output')(x)\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# EEGNet Model\n",
        "def build_eegnet_model(input_shape, nb_classes=5, chans=2, samples=1500, dropout_rate=0.25, kern_length=64, f1=8, d=2, f2=16):\n",
        "    input1 = layers.Input(shape=(samples, chans))\n",
        "    x = layers.Lambda(lambda t: tf.expand_dims(t, axis=-1))(input1)  # Fixed with Lambda\n",
        "    x = layers.Permute((2, 1, 3))(x)\n",
        "\n",
        "    x = layers.Conv2D(f1, (1, kern_length), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.DepthwiseConv2D((chans, 1), use_bias=False, depth_multiplier=d, depthwise_constraint=constraints.max_norm(1.0))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.AveragePooling2D((1, 4))(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.SeparableConv2D(f2, (1, 16), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.AveragePooling2D((1, 8))(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu', name='dense_before_dropout')(x)\n",
        "    x = layers.Dropout(0.3, name='final_dropout')(x)\n",
        "    outputs = layers.Dense(nb_classes, activation='softmax', dtype='float32', name='output')(x)\n",
        "\n",
        "    model = models.Model(inputs=input1, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Batched prediction function\n",
        "def batched_predict(model, X, batch_size=BATCH_SIZE, training=False):\n",
        "    preds = []\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch = X[i:i+batch_size]\n",
        "        pred = model(batch, training=training)\n",
        "        preds.append(pred.numpy())\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "# Monte Carlo Dropout\n",
        "def build_mc_dropout_model(base_builder, input_shape, nb_classes=5, dropout_rate=0.25):\n",
        "    model = base_builder(input_shape, nb_classes)\n",
        "    return model\n",
        "\n",
        "def predict_with_mc_dropout(model, X, n_samples=20, batch_size=BATCH_SIZE):\n",
        "    preds = []\n",
        "    for _ in range(n_samples):\n",
        "        pred = batched_predict(model, X, batch_size, training=True)\n",
        "        preds.append(pred)\n",
        "    preds = np.stack(preds, axis=0)\n",
        "    mean_pred = np.mean(preds, axis=0)\n",
        "    uncertainty = np.std(preds, axis=0)\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Deep Ensembles\n",
        "def train_deep_ensembles(base_builder, input_shape, nb_classes, train_gen, val_gen, n_ensembles=5):\n",
        "    ensembles = []\n",
        "    for _ in range(n_ensembles):\n",
        "        model = base_builder(input_shape, nb_classes)\n",
        "        model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, verbose=1)\n",
        "        ensembles.append(model)\n",
        "    return ensembles\n",
        "\n",
        "def predict_with_deep_ensembles(ensembles, X, batch_size=BATCH_SIZE):\n",
        "    preds = []\n",
        "    for model in ensembles:\n",
        "        pred = batched_predict(model, X, batch_size)\n",
        "        preds.append(pred)\n",
        "    preds = np.stack(preds, axis=0)\n",
        "    mean_pred = np.mean(preds, axis=0)\n",
        "    uncertainty = np.std(preds, axis=0)\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Variational Inference\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_fn = tfp.distributions.MultivariateNormalDiag(\n",
        "        loc=tf.zeros(n, dtype=dtype),\n",
        "        scale_diag=tf.ones(n, dtype=dtype))\n",
        "    return prior_fn\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    return tf.keras.Sequential([\n",
        "        tfp.layers.VariableLayer(tfp.distributions.MultivariateNormalTriL.params_size(n), dtype=dtype),\n",
        "        tfp.layers.MultivariateNormalTriL(n)\n",
        "    ])\n",
        "\n",
        "def build_variational_model(base_builder, input_shape, nb_classes, train_size):\n",
        "    base_model = base_builder(input_shape, nb_classes)\n",
        "    # Get the output before the final dropout layer\n",
        "    x = base_model.get_layer('dense_before_dropout').output\n",
        "    x = tfp.layers.DenseVariational(\n",
        "        units=nb_classes,\n",
        "        make_prior_fn=prior,\n",
        "        make_posterior_fn=posterior,\n",
        "        kl_weight=1/train_size,\n",
        "        activation='softmax',\n",
        "        dtype=tf.float32\n",
        "    )(x)\n",
        "    model = models.Model(inputs=base_model.input, outputs=x)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def predict_with_variational(model, X, n_samples=20, batch_size=BATCH_SIZE):\n",
        "    preds = []\n",
        "    for _ in range(n_samples):\n",
        "        pred = batched_predict(model, X, batch_size)\n",
        "        preds.append(pred)\n",
        "    preds = np.stack(preds, axis=0)\n",
        "    mean_pred = np.mean(preds, axis=0)\n",
        "    uncertainty = np.std(preds, axis=0)\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Evidential Deep Learning\n",
        "def build_evidential_model(base_builder, input_shape, nb_classes):\n",
        "    base_model = base_builder(input_shape, nb_classes)\n",
        "    x = base_model.get_layer('dense_before_dropout').output\n",
        "    evidence = layers.Dense(nb_classes)(x)\n",
        "    outputs = layers.Activation('relu', dtype='float32')(evidence)\n",
        "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss=evidential_classification_loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def predict_with_evidential(model, X, batch_size=BATCH_SIZE):\n",
        "    evidence = batched_predict(model, X, batch_size)\n",
        "    alpha = evidence + 1\n",
        "    S = np.sum(alpha, axis=1, keepdims=True)\n",
        "    mean_pred = alpha / S\n",
        "    uncertainty = nb_classes / S\n",
        "    return mean_pred, uncertainty\n",
        "\n",
        "# Uncertainty Metrics\n",
        "def negative_log_likelihood(y_true, y_pred):\n",
        "    y_true_onehot = tf.keras.utils.to_categorical(y_true, num_classes=y_pred.shape[1])\n",
        "    return -np.mean(np.sum(y_true_onehot * np.log(y_pred + 1e-10), axis=1))\n",
        "\n",
        "def brier_score(y_true, y_pred):\n",
        "    y_true_onehot = tf.keras.utils.to_categorical(y_true, num_classes=y_pred.shape[1])\n",
        "    return np.mean(np.sum((y_pred - y_true_onehot)**2, axis=1))\n",
        "\n",
        "def expected_calibration_error(y_true, y_pred, num_bins=15):\n",
        "    pred_classes = np.argmax(y_pred, axis=1)\n",
        "    prob_max = np.max(y_pred, axis=1)\n",
        "    correct = (pred_classes == y_true).astype(np.float32)\n",
        "\n",
        "    bins = np.linspace(0, 1, num_bins + 1)\n",
        "    indices = np.digitize(prob_max, bins, right=True)\n",
        "\n",
        "    ece = 0\n",
        "    for b in range(1, num_bins + 1):\n",
        "        mask = (indices == b)\n",
        "        if np.any(mask):\n",
        "            acc = np.mean(correct[mask])\n",
        "            conf = np.mean(prob_max[mask])\n",
        "            ece += np.abs(acc - conf) * np.sum(mask) / len(y_true)\n",
        "    return ece\n",
        "\n",
        "def plot_training_curves(history, model_name):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_curves_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_model(model_name, predict_func, X_test, y_test):\n",
        "    mean_pred, uncertainty = predict_func(X_test)\n",
        "    test_acc = np.mean(np.argmax(mean_pred, axis=1) == y_test)\n",
        "    nll = negative_log_likelihood(y_test, mean_pred)\n",
        "    brier = brier_score(y_test, mean_pred)\n",
        "    ece = expected_calibration_error(y_test, mean_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} - Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"{model_name} - NLL: {nll:.4f}\")\n",
        "    print(f\"{model_name} - Brier Score: {brier:.4f}\")\n",
        "    print(f\"{model_name} - ECE: {ece:.4f}\")\n",
        "\n",
        "    y_pred_classes = np.argmax(mean_pred, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_classes, average=None)\n",
        "    stage_names = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
        "    print(f\"\\n{model_name} - Per-class Metrics:\")\n",
        "    for i, stage in enumerate(stage_names):\n",
        "        print(f\"{stage}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}\")\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=stage_names, yticklabels=stage_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.savefig(f'confusion_matrix_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return {'acc': test_acc, 'nll': nll, 'brier': brier, 'ece': ece}\n",
        "\n",
        "def data_generator(available, batch_size=2000):\n",
        "    for subject_id, night in available:\n",
        "        X, y = process_subject_night(subject_id, night)\n",
        "        if X is None or y is None:\n",
        "            continue\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "        del X, y\n",
        "        gc.collect()\n",
        "\n",
        "def run_pipeline():\n",
        "    available = get_available_subjects()\n",
        "    if not available:\n",
        "        return\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "    for X_batch, y_batch in tqdm(data_generator(available), desc=\"Processing data\"):\n",
        "        if X_batch is None or y_batch is None:\n",
        "            continue\n",
        "        class_counts = np.bincount(y_batch)\n",
        "        stratify = y_batch if min(class_counts[class_counts > 0]) >= 2 else None\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X_batch, y_batch, test_size=0.2, stratify=stratify, random_state=42)\n",
        "        X_train.append(X_tr); y_train.append(y_tr)\n",
        "        X_test.append(X_te); y_test.append(y_te)\n",
        "        del X_batch, y_batch\n",
        "        gc.collect()\n",
        "    if not X_train:\n",
        "        return\n",
        "    X_train = np.concatenate(X_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    X_test = np.concatenate(X_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "    train_generator = DataGenerator(X_train, y_train, BATCH_SIZE, augment=True, class_weights=class_weight_dict)\n",
        "    val_generator = DataGenerator(X_test, y_test, BATCH_SIZE, augment=False, class_weights=class_weight_dict)\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    nb_classes = 5\n",
        "    train_size = len(X_train)\n",
        "\n",
        "    # Order: EEGNet first, then LSTM\n",
        "    base_builders = [('EEGNet', build_eegnet_model), ('LSTM', build_lstm_model)]\n",
        "    uncertainty_methods = [\n",
        "        ('MC_Dropout', build_mc_dropout_model, lambda model, X: predict_with_mc_dropout(model, X)),\n",
        "        ('Deep_Ensembles', train_deep_ensembles, lambda ensembles, X: predict_with_deep_ensembles(ensembles, X)),\n",
        "        ('Variational_Inference', lambda bs, ish, nbc: build_variational_model(bs, ish, nbc, train_size), lambda model, X: predict_with_variational(model, X)),\n",
        "        ('Evidential_DL', build_evidential_model, lambda model, X: predict_with_evidential(model, X))\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for base_name, base_builder in base_builders:\n",
        "        for unc_name, build_func, predict_func in uncertainty_methods:\n",
        "            model_name = f\"{base_name}_{unc_name}\"\n",
        "            print(f\"\\nTraining {model_name}...\")\n",
        "            if unc_name == 'Deep_Ensembles':\n",
        "                models = build_func(base_builder, input_shape, nb_classes, train_generator, val_generator)\n",
        "                metrics = evaluate_model(model_name, lambda X: predict_func(models, X), X_test, y_test)\n",
        "            else:\n",
        "                model = build_func(base_builder, input_shape, nb_classes)\n",
        "                history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "                plot_training_curves(history, model_name)\n",
        "                metrics = evaluate_model(model_name, lambda X: predict_func(model, X), X_test, y_test)\n",
        "            results[model_name] = metrics\n",
        "\n",
        "    # Compare\n",
        "    print(\"\\nModel Comparison for Uncertainty Estimation:\")\n",
        "    print(\"Sorted by ECE (lower is better):\")\n",
        "    for name, met in sorted(results.items(), key=lambda x: x[1]['ece']):\n",
        "        print(f\"{name}: Acc={met['acc']:.4f}, NLL={met['nll']:.4f}, Brier={met['brier']:.4f}, ECE={met['ece']:.4f}\")\n",
        "\n",
        "    best_model = min(results, key=lambda k: results[k]['ece'])\n",
        "    print(f\"\\nBest fit model for uncertainty estimation: {best_model} with ECE {results[best_model]['ece']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "w2nUGOmuLKIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training EEGNet_MC_Dropout...\n",
        "I0000 00:00:1753103013.421626   29909 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9797 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:d9:00.0, compute capability: 7.5\n",
        "/home/samibc/miniconda3/envs/tf-mnist/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
        "  self._warn_if_super_not_called()\n",
        "Epoch 1/3\n",
        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
        "I0000 00:00:1753103016.817587   30196 service.cc:148] XLA service 0x7f012c006080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
        "I0000 00:00:1753103016.817628   30196 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
        "2025-07-21 09:03:36.871935: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
        "I0000 00:00:1753103017.212053   30196 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
        "2025-07-21 09:03:38.083020: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=0,k4=2,k5=3,k6=3,k7=2} for conv (f16[16,1,375,16]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,1,375,16]{3,2,1,0}, f16[16,1,1,16]{3,2,1,0}), window={size=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
        "2025-07-21 09:03:38.085995: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=4,k6=3,k7=2} for conv (f16[16,1,375,16]{3,2,1,0}, u8[0]{0}) custom-call(f16[16,1,375,16]{3,2,1,0}, f16[16,1,1,16]{3,2,1,0}), window={size=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
        "I0000 00:00:1753103021.301281   30196 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
        "2965/5956 ━━━━━━━━━━━━━━━━━━━━ 10s 4ms/step - accuracy: 0.5192 - loss: 1.25312025-07-21 09:03:52.509675: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=0,k4=2,k5=3,k6=3,k7=2} for conv (f16[10,1,375,16]{3,2,1,0}, u8[0]{0}) custom-call(f16[10,1,375,16]{3,2,1,0}, f16[16,1,1,16]{3,2,1,0}), window={size=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
        "2025-07-21 09:03:52.510641: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=4,k6=3,k7=2} for conv (f16[10,1,375,16]{3,2,1,0}, u8[0]{0}) custom-call(f16[10,1,375,16]{3,2,1,0}, f16[16,1,1,16]{3,2,1,0}), window={size=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
        "5950/5956 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.5995 - loss: 1.09872025-07-21 09:04:10.519594: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=0,k4=2,k5=3,k6=3,k7=2} for conv (f16[8,1,375,16]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,1,375,16]{3,2,1,0}, f16[16,1,1,16]{3,2,1,0}), window={size=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
        "2025-07-21 09:04:10.520825: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=4,k6=3,k7=2} for conv (f16[8,1,375,16]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,1,375,16]{3,2,1,0}, f16[16,1,1,16]{3,2,1,0}), window={size=1x1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 36s 5ms/step - accuracy: 0.5996 - loss: 1.0985 - val_accuracy: 0.8028 - val_loss: 0.6513\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 25s 4ms/step - accuracy: 0.7848 - loss: 0.7016 - val_accuracy: 0.8049 - val_loss: 0.5798\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 24s 4ms/step - accuracy: 0.8194 - loss: 0.6254 - val_accuracy: 0.7102 - val_loss: 0.6785\n",
        "\n",
        "EEGNet_MC_Dropout - Test Accuracy: 0.8250\n",
        "EEGNet_MC_Dropout - NLL: 0.5071\n",
        "EEGNet_MC_Dropout - Brier Score: 0.2587\n",
        "EEGNet_MC_Dropout - ECE: 0.0335\n",
        "\n",
        "EEGNet_MC_Dropout - Per-class Metrics:\n",
        "Wake: Precision=0.9887, Recall=0.8695, F1=0.9253\n",
        "N1: Precision=0.1930, Recall=0.7820, F1=0.3096\n",
        "N2: Precision=0.8860, Recall=0.7462, F1=0.8101\n",
        "N3: Precision=0.7226, Recall=0.8884, F1=0.7970\n",
        "REM: Precision=0.6654, Recall=0.6640, F1=0.6647\n",
        "\n",
        "Training EEGNet_Deep_Ensembles...\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 34s 5ms/step - accuracy: 0.6337 - loss: 1.0680 - val_accuracy: 0.7219 - val_loss: 0.6701\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 24s 4ms/step - accuracy: 0.7967 - loss: 0.6582 - val_accuracy: 0.7746 - val_loss: 0.5864\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 24s 4ms/step - accuracy: 0.8199 - loss: 0.6122 - val_accuracy: 0.7599 - val_loss: 0.6847\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 39s 6ms/step - accuracy: 0.6376 - loss: 1.0771 - val_accuracy: 0.7760 - val_loss: 0.6309\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 25s 4ms/step - accuracy: 0.7876 - loss: 0.7005 - val_accuracy: 0.7277 - val_loss: 0.6560\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 26s 4ms/step - accuracy: 0.8186 - loss: 0.6303 - val_accuracy: 0.7902 - val_loss: 0.6194\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 40s 6ms/step - accuracy: 0.6262 - loss: 1.0699 - val_accuracy: 0.8087 - val_loss: 0.6183\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 25s 4ms/step - accuracy: 0.7944 - loss: 0.6806 - val_accuracy: 0.7907 - val_loss: 0.5879\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 25s 4ms/step - accuracy: 0.8186 - loss: 0.6189 - val_accuracy: 0.8129 - val_loss: 0.6113\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 35s 5ms/step - accuracy: 0.6177 - loss: 1.0797 - val_accuracy: 0.7500 - val_loss: 0.6736\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 26s 4ms/step - accuracy: 0.7914 - loss: 0.6806 - val_accuracy: 0.7249 - val_loss: 0.6463\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 26s 4ms/step - accuracy: 0.8117 - loss: 0.6192 - val_accuracy: 0.7474 - val_loss: 0.6671\n",
        "Epoch 1/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 37s 5ms/step - accuracy: 0.6353 - loss: 1.0861 - val_accuracy: 0.7783 - val_loss: 0.6230\n",
        "Epoch 2/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 25s 4ms/step - accuracy: 0.7922 - loss: 0.6771 - val_accuracy: 0.8027 - val_loss: 0.5811\n",
        "Epoch 3/3\n",
        "5956/5956 ━━━━━━━━━━━━━━━━━━━━ 26s 4ms/step - accuracy: 0.8137 - loss: 0.6343 - val_accuracy: 0.7982 - val_loss: 0.6387\n",
        "\n",
        "EEGNet_Deep_Ensembles - Test Accuracy: 0.7990\n",
        "EEGNet_Deep_Ensembles - NLL: 0.5627\n",
        "EEGNet_Deep_Ensembles - Brier Score: 0.2846\n",
        "EEGNet_Deep_Ensembles - ECE: 0.0236\n",
        "\n",
        "EEGNet_Deep_Ensembles - Per-class Metrics:\n",
        "Wake: Precision=0.9987, Recall=0.8359, F1=0.9101\n",
        "N1: Precision=0.2044, Recall=0.6371, F1=0.3095\n",
        "N2: Precision=0.9425, Recall=0.6449, F1=0.7658\n",
        "N3: Precision=0.5224, Recall=0.9757, F1=0.6804\n",
        "REM: Precision=0.5483, Recall=0.8281, F1=0.6597"
      ],
      "metadata": {
        "id": "GZuZ3xElLK0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STXCEn3XLyGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}